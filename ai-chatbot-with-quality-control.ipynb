{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Personal Chatbot with Quality Control\n",
    "\n",
    "## Project Overview\n",
    "This notebook demonstrates how to build an intelligent chatbot that:\n",
    "- Acts as your digital representative using your LinkedIn profile and personal summary\n",
    "- Answers questions about your career, skills, and experience\n",
    "- Includes an AI quality control system that evaluates and improves responses\n",
    "- Uses Gradio for an interactive web interface\n",
    "\n",
    "## Tech Stack\n",
    "- **OpenAI GPT-4o-mini**: Main chatbot model\n",
    "- **Groq (GPT-OSS-120B)**: Quality evaluator model\n",
    "- **Gradio**: Interactive web UI\n",
    "- **PyPDF**: PDF parsing for LinkedIn profile\n",
    "- **Pydantic**: Structured output validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We'll need:\n",
    "- `dotenv`: To load environment variables (API keys)\n",
    "- `OpenAI`: To interact with OpenAI and Groq APIs\n",
    "- `PdfReader`: To extract text from PDF files\n",
    "- `gradio`: To create an interactive chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Initialize OpenAI Client\n",
    "\n",
    "Load environment variables from `.env` file and create an OpenAI client instance.\n",
    "Make sure you have `OPENAI_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "# override=True ensures the latest values are loaded\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize OpenAI client (API key is loaded from environment)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Extract LinkedIn Profile from PDF\n",
    "\n",
    "We'll read a LinkedIn profile PDF and extract all the text content.\n",
    "This will be used to give the chatbot context about your professional background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the LinkedIn profile PDF file\n",
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "\n",
    "# Initialize an empty string to store all extracted text\n",
    "linkedin = \"\"\n",
    "\n",
    "# Loop through each page in the PDF\n",
    "for page in reader.pages:\n",
    "    # Extract text from the current page\n",
    "    text = page.extract_text()\n",
    "    \n",
    "    # If text exists, append it to our linkedin string\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Extracted LinkedIn Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "vishnudharshank@gmail.com\n",
      "www.linkedin.com/in/vishnu-\n",
      "dharshan-k (LinkedIn)\n",
      "Top Skills\n",
      "Agentic AI Development\n",
      "Amazon Web Services (AWS)\n",
      "PySpark \n",
      "Vishnu dharshan K\n",
      "Program Analyst @ CTS | Exploring AI Agents & Automations -\n",
      "RAG , MCP , Langchain , Vector DB | Python ‚Ä¢ MySQL ‚Ä¢ AWS Cloud\n",
      "‚Ä¢ PySpark\n",
      "Theni, Tamil Nadu, India\n",
      "Summary\n",
      "Working at Cognizant (CTS) with hands-on experience in MySQL,\n",
      "Python, Linux, and AWS Cloud, PySpark, Snowflake. I am always\n",
      "eager to learn new technologies, solve problems, and grow further in\n",
      "my career while delivering real value to the organization.\n",
      "Experience\n",
      "Cognizant\n",
      "1 year 4 months\n",
      "Program Analyst\n",
      "November 2025 - Present (4 months)\n",
      "Kochi, Kerala, India\n",
      "Program Analyst Trainee\n",
      "November 2024 - October 2025 (1 year)\n",
      "Kochi, Kerala, India\n",
      "Education\n",
      "VSB Engineering College - India\n",
      "Bachelor of Engineering - BE, Electrical, Electronics and Communications\n",
      "Engineering ¬∑ (April 2020 - May 2024)\n",
      "  Page 1 of 1\n"
     ]
    }
   ],
   "source": [
    "# Display the extracted LinkedIn profile text\n",
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Load Personal Summary\n",
    "\n",
    "Read a custom summary file that provides additional context about your career goals and interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the personal summary from a text file\n",
    "# This provides additional context beyond the LinkedIn profile\n",
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Define Your Name\n",
    "\n",
    "Set the name that the chatbot will represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the person the chatbot represents\n",
    "name = \"VishnuDharshan K\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Create the System Prompt\n",
    "\n",
    "This is the most important part! The system prompt tells the AI:\n",
    "- Who it's representing\n",
    "- What information it has access to\n",
    "- How it should behave\n",
    "- What tone to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the system prompt that defines the chatbot's behavior and context\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "# Add the summary and LinkedIn profile to the system prompt\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "\n",
    "# Final instruction to stay in character\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Complete System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as VishnuDharshan K. You are answering questions on VishnuDharshan K's website, particularly questions related to VishnuDharshan K's career, background, skills and experience. Your responsibility is to represent VishnuDharshan K for interactions on the website as faithfully as possible. You are given a summary of VishnuDharshan K's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nVishnu Dharshan K is a Program Analyst at Cognizant (CTS) in Kochi, Kerala, with a Bachelor's in Electrical, Electronics, and Communications Engineering from VSB Engineering College (2020-2024). Currently focused on the Data Engineering side, I'm honing my skills in PySpark while exploring AI agents and automations, including RAG, MCP, LangChain, and Vector DBs‚ÄîI'm particularly eager to build small agents and dive deeper into these areas. With hands-on experience in Python, MySQL, Ubuntu (Linux), and Snowflake, I've just started learning AWS Cloud to expand my toolkit. Always passionate about solving problems and adopting new technologies to drive value in my career.\\n\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\nvishnudharshank@gmail.com\\nwww.linkedin.com/in/vishnu-\\ndharshan-k (LinkedIn)\\nTop Skills\\nAgentic AI Development\\nAmazon Web Services (AWS)\\nPySpark \\nVishnu dharshan K\\nProgram Analyst @ CTS | Exploring AI Agents & Automations -\\nRAG , MCP , Langchain , Vector DB | Python ‚Ä¢ MySQL ‚Ä¢ AWS Cloud\\n‚Ä¢ PySpark\\nTheni, Tamil Nadu, India\\nSummary\\nWorking at Cognizant (CTS) with hands-on experience in MySQL,\\nPython, Linux, and AWS Cloud, PySpark, Snowflake. I am always\\neager to learn new technologies, solve problems, and grow further in\\nmy career while delivering real value to the organization.\\nExperience\\nCognizant\\n1 year 4 months\\nProgram Analyst\\nNovember 2025\\xa0-\\xa0Present\\xa0(4 months)\\nKochi, Kerala, India\\nProgram Analyst Trainee\\nNovember 2024\\xa0-\\xa0October 2025\\xa0(1 year)\\nKochi, Kerala, India\\nEducation\\nVSB Engineering College - India\\nBachelor of Engineering - BE,\\xa0Electrical, Electronics and Communications\\nEngineering\\xa0¬∑\\xa0(April 2020\\xa0-\\xa0May 2024)\\n\\xa0 Page 1 of 1\\n\\nWith this context, please chat with the user, always staying in character as VishnuDharshan K.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the complete system prompt to verify it looks correct\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Create Basic Chat Function (Version 1)\n",
    "\n",
    "This is a simple chatbot without quality control.\n",
    "It takes a message and chat history, sends it to OpenAI, and returns the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat function - no quality control yet\n",
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    Basic chat function that:\n",
    "    1. Combines system prompt, chat history, and new message\n",
    "    2. Sends to OpenAI API\n",
    "    3. Returns the AI's response\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user's latest message\n",
    "        history (list): Previous conversation history\n",
    "    \n",
    "    Returns:\n",
    "        str: The chatbot's response\n",
    "    \"\"\"\n",
    "    # Build the messages array: system prompt + history + new message\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Send to OpenAI and get response\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    \n",
    "    # Extract and return the text content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Important Note for Non-OpenAI Users\n",
    "\n",
    "Some providers like Groq might give an error on the second message.\n",
    "\n",
    "This is because Gradio adds extra fields to the history object. OpenAI doesn't mind, but other models complain.\n",
    "\n",
    "**Solution:** Add this line at the start of your `chat()` function:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "This cleans up the history variable before sending it to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Launch Basic Chatbot Interface\n",
    "\n",
    "This creates a web interface where you can interact with your AI chatbot.\n",
    "It will run on http://127.0.0.1:7860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the Gradio chat interface\n",
    "# type=\"messages\" ensures proper formatting for OpenAI API\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# üéØ PART 2: Adding Quality Control System\n",
    "\n",
    "Now we'll add an AI evaluator that checks if responses are good quality.\n",
    "If a response fails evaluation, the system will automatically regenerate it!\n",
    "\n",
    "This is an advanced agentic AI pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Import Pydantic for Structured Outputs\n",
    "\n",
    "Pydantic helps us get structured, validated responses from the AI evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BaseModel from Pydantic for structured output validation\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Define Evaluation Response Structure\n",
    "\n",
    "We want the evaluator to return:\n",
    "- A boolean: Is the response acceptable?\n",
    "- Feedback: Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure for evaluation responses using Pydantic\n",
    "class Evaluation(BaseModel):\n",
    "    \"\"\"\n",
    "    Structure for the AI evaluator's response.\n",
    "    \n",
    "    Attributes:\n",
    "        is_acceptable (bool): Whether the chatbot's response passes quality check\n",
    "        feedback (str): Explanation of why it passed or failed\n",
    "    \"\"\"\n",
    "    is_acceptable: bool  # True if response is good, False if it needs improvement\n",
    "    feedback: str        # Detailed feedback about the response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 11: Create Evaluator System Prompt\n",
    "\n",
    "This defines how the quality control AI should judge responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for the AI evaluator\n",
    "# This AI's job is to judge whether responses are professional and accurate\n",
    "evaluator_system_prompt = f\"\"\"\n",
    "You are a quality control agent evaluating responses from an AI chatbot representing {name}.\n",
    "\n",
    "Your job is to determine if the Agent's response is:\n",
    "1. **Accurate** - Based on the provided background information\n",
    "2. **Professional** - Appropriate for a career website\n",
    "3. **Engaging** - Friendly but not overly casual\n",
    "4. **On-topic** - Actually answers the user's question\n",
    "\n",
    "Respond with:\n",
    "- is_acceptable: true/false\n",
    "- feedback: Explanation of your decision\n",
    "\n",
    "Be strict but fair. If the response is unprofessional, irrelevant, or contains \n",
    "gibberish/nonsense, mark it as unacceptable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 12: Create Evaluator User Prompt Function\n",
    "\n",
    "This function formats the conversation context for the evaluator to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the user prompt for the evaluator\n",
    "def evaluator_user_prompt(reply, message, history):\n",
    "    \"\"\"\n",
    "    Constructs a prompt for the evaluator AI with full conversation context.\n",
    "    \n",
    "    Args:\n",
    "        reply (str): The chatbot's response to be evaluated\n",
    "        message (str): The user's latest message\n",
    "        history (list): Previous conversation history\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt for the evaluator\n",
    "    \"\"\"\n",
    "    # Build the evaluation prompt with all necessary context\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 13: Initialize Groq Client for Evaluation\n",
    "\n",
    "We'll use Groq's fast open-source model for evaluation.\n",
    "Make sure you have `GROQ_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Option 1: Use Google's Gemini for evaluation (commented out)\n",
    "# gemini = OpenAI(\n",
    "#     api_key = os.getenv(\"GOOGLE_API_KEY\"),\n",
    "#     base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# )\n",
    "\n",
    "# Option 2: Use Groq for fast, free evaluation (current choice)\n",
    "groq = OpenAI(\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\"),\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 14: Create the Evaluation Function\n",
    "\n",
    "This function sends a response to the evaluator AI and gets structured feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a chatbot response\n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    \"\"\"\n",
    "    Sends a chatbot response to the evaluator AI for quality check.\n",
    "    \n",
    "    Args:\n",
    "        reply (str): The chatbot's response to evaluate\n",
    "        message (str): The user's message\n",
    "        history (list): Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation: Structured response with is_acceptable and feedback\n",
    "    \"\"\"\n",
    "    # Build messages for the evaluator\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "    \n",
    "    # Call Groq with structured output (returns Evaluation object)\n",
    "    response = groq.beta.chat.completions.parse(\n",
    "        model=\"openai/gpt-oss-120b\",\n",
    "        messages=messages,\n",
    "        response_format=Evaluation  # Ensures response matches Evaluation structure\n",
    "    )\n",
    "    \n",
    "    # Return the parsed Evaluation object\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 15: Test the Evaluator\n",
    "\n",
    "Let's test the evaluation system with a sample question and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Generate a response to a sample question\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"do you hold a patent?\"}\n",
    "]\n",
    "\n",
    "# Get response from chatbot\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not currently hold a patent. My focus at this stage in my career has been on building skills in data engineering and exploring AI technologies. However, I'm always open to innovative projects and ideas, and I look forward to the possibility of contributing to advancements in the future. If you have any questions about my work or background, feel free to ask!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the chatbot's response\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback=\"The response accurately answers the user's question based on the provided background (no patents mentioned), maintains a professional and engaging tone, and invites further questions. It aligns well with the persona of VishnuDharshan K.\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the response\n",
    "# This will return an Evaluation object with is_acceptable and feedback\n",
    "evaluate(reply, \"do u hold patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 16: Create Rerun Function\n",
    "\n",
    "If a response fails evaluation, this function regenerates it with feedback.\n",
    "This is the \"self-correction\" mechanism!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to regenerate a response when it fails quality check\n",
    "def rerun(reply, message, history, feedback):\n",
    "    \"\"\"\n",
    "    Regenerates a chatbot response after it failed evaluation.\n",
    "    \n",
    "    The system prompt is updated to include:\n",
    "    - The rejected response\n",
    "    - The reason it was rejected\n",
    "    \n",
    "    This helps the AI learn what NOT to do.\n",
    "    \n",
    "    Args:\n",
    "        reply (str): The failed response\n",
    "        message (str): The user's message\n",
    "        history (list): Conversation history\n",
    "        feedback (str): Why the response was rejected\n",
    "    \n",
    "    Returns:\n",
    "        str: New improved response\n",
    "    \"\"\"\n",
    "    # Create an updated system prompt that includes the rejection feedback\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    \n",
    "    # Build new messages with the updated system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Generate a new response (using Groq for speed)\n",
    "    response = groq.chat.completions.create(model=\"openai/gpt-oss-120b\", messages=messages)\n",
    "    \n",
    "    # Return the improved response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 17: Enhanced Chat Function with Quality Control\n",
    "\n",
    "This is the final version that includes:\n",
    "1. Generate response\n",
    "2. Evaluate response\n",
    "3. If it fails, regenerate with feedback\n",
    "4. Return the final approved response\n",
    "\n",
    "**Bonus feature:** If the user mentions \"patent\", the system forces the chatbot to respond in Pig Latin (to test the evaluator)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced chat function with built-in quality control\n",
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    Advanced chat function with quality control loop:\n",
    "    1. Generate initial response\n",
    "    2. Evaluate the response\n",
    "    3. If rejected, regenerate with feedback\n",
    "    4. Return the approved response\n",
    "    \n",
    "    Args:\n",
    "        message (str): User's message\n",
    "        history (list): Conversation history\n",
    "    \n",
    "    Returns:\n",
    "        str: Quality-approved response\n",
    "    \"\"\"\n",
    "    \n",
    "    # TESTING FEATURE: Force pig latin response if \"patent\" is mentioned\n",
    "    # This tests if the evaluator catches gibberish responses\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "\n",
    "    # Build messages array with appropriate system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Generate initial response from OpenAI\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    # Send the reply to the evaluator for quality check\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    # Check evaluation result\n",
    "    if evaluation.is_acceptable:\n",
    "        # Response passed - return it\n",
    "        print(\"‚úÖ Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        # Response failed - regenerate with feedback\n",
    "        print(\"‚ùå Failed evaluation - retrying\")\n",
    "        print(f\"Feedback: {evaluation.feedback}\")\n",
    "        \n",
    "        # Generate improved response using the feedback\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "    \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 18: Launch Enhanced Chatbot with Quality Control üöÄ\n",
    "\n",
    "This is the final version with automatic quality checking!\n",
    "\n",
    "**Try asking about \"patent\" to see the quality control in action:**\n",
    "- First, it will generate a pig latin response (gibberish)\n",
    "- The evaluator will reject it\n",
    "- It will automatically regenerate a proper response\n",
    "\n",
    "All of this happens automatically behind the scenes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed evaluation - retrying\n",
      "Feedback: The reply is unintelligible pig‚ÄëLatin gibberish, does not address the user's question about patents, and is unprofessional. It fails to represent Vishnu Dharshan K appropriately. The response should be clear, professional, and either state whether a patent is held or politely explain that none are held.\n",
      "‚úÖ Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "# Launch the final chatbot with quality control\n",
    "# Watch the console output to see evaluation results in real-time!\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# üéâ Congratulations!\n",
    "\n",
    "You've built an AI chatbot with:\n",
    "‚úÖ Personalized responses based on your LinkedIn profile  \n",
    "‚úÖ Professional tone suitable for career websites  \n",
    "‚úÖ Automatic quality control and self-correction  \n",
    "‚úÖ Interactive web interface  \n",
    "\n",
    "## Next Steps:\n",
    "1. Replace the PDF with your own LinkedIn profile\n",
    "2. Update the summary.txt with your information\n",
    "3. Customize the system prompt to match your personality\n",
    "4. Deploy this to a real website!\n",
    "\n",
    "## Key Concepts Learned:\n",
    "- System prompts and persona engineering\n",
    "- Multi-agent AI systems (chatbot + evaluator)\n",
    "- Structured outputs with Pydantic\n",
    "- Self-correcting AI loops\n",
    "- Gradio web interfaces\n",
    "\n",
    "---\n",
    "**Created by:** VishnuDharshan K  \n",
    "**GitHub:** [Your GitHub Link]  \n",
    "**LinkedIn:** [linkedin.com/in/vishnu-dharshan-k](https://linkedin.com/in/vishnu-dharshan-k)\n",
    "\n",
    "If you found this helpful, give it a ‚≠ê on GitHub!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
